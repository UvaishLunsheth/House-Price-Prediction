print('In The Name Of Allah The Most Gracious The Most Merciful')


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import (accuracy_score, precision_score, confusion_matrix, 
                            f1_score, recall_score, classification_report, 
                            roc_curve, auc, precision_recall_curve, roc_auc_score)


# Load the dataset
df = pd.read_csv('housing.csv')


def data_summary(df):
    print("📌 Dataset Shape:")
    print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
    print("\n📌 Column-wise Missing Values:")
    missing = df.isnull().sum()
    if missing.any():
        print(missing[missing > 0])
    else:
        print("✅ No missing values found.")
    
    print("\n📌 Duplicate Rows:")
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        print(f"⚠️ Found {duplicate_count} duplicate rows.")
    else:
        print("✅ No duplicate rows found.")
    
    print("\n📌 Basic Info:")
    print(df.info())

# Call it
data_summary(df)


# Display statistical summary
print("📊 Statistical Summary of Numerical Features:")
df.describe().T.style.background_gradient(cmap='twilight_shifted')


# identify the  numeric columns
numeric_df = df.select_dtypes(include='number')

# Show only highly skewed columns
print("\nSkewd Columns")
skewness = numeric_df.skew().sort_values(ascending=False)
print(skewness)

# Show columns with skewness > 1 or < -1 (highly skewed)
print("\nHighly Skewed Columns:")
print(skewness[abs(skewness) > 1])



print("\n🚨 Potential Outliers (using Z-score > 3):")
from scipy.stats import zscore
z_scores = df.select_dtypes(include='number').apply(zscore)
outliers = (abs(z_scores) > 3).sum()
print(outliers)





# Identify numerical features
feature = df.select_dtypes(include='number').columns.tolist()

# Exclude the "heart_disease" target column
numeric_features = [col for col in feature if col != 'ocean_proximity']

# Create subplot grid
num_cols = len(numeric_features)
n_rows = (num_cols + 2) // 3  # 3 columns per row
fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 4))
axes = axes.flatten()

# Plot each numeric feature
for i, col in enumerate(numeric_features):
    sns.histplot(data=df,x=col,palette='Set3', ax=axes[i])
    axes[i].set_title(f"{col.replace('_',' ').capitalize()} Distribution ")
    axes[i].set_xlabel(f"{col.replace('_',' ').capitalize()}")
    axes[i].set_ylabel('Count')
    sns.despine(ax=axes[i], top=True, right=True, left=True)

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()
plt.savefig("Numeric Feature Distribution of Housing.png")





df['total_bedrooms'] = df.groupby('ocean_proximity')['total_bedrooms'].transform(lambda x : x.fillna(x.median()))











for col in ['population', 'total_rooms', 'households', 'median_income']:
    q = df[col].quantile(0.99)
    df[col] = np.where(df[col]>q,q,df[col])











# Select only numeric features (including target for reference)
numeric_df = df.select_dtypes(include='number')

# Compute correlation matrix
corr_matrix = numeric_df.corr()

# Set up the plot
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5)

plt.title("Correlation matrix and Heatmap of Numeric Features", fontsize=16)
plt.xticks(rotation=90, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
# plt.savefig("Correlation matrix and Heatmap of Numeric Features.png")





df['income_cat'] = pd.cut(
    df['median_income'],
    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
    labels=[1, 2, 3, 4, 5])

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(df, df["income_cat"]):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]









for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)



housing = strat_train_set.copy()





from PIL import Image

# Load using Pillow
img = Image.open("california-map2.jpeg")

# set up the figure
plt.figure(figsize=(12,8))

# adjust extent to match long/lat range of california
# tweak the value based on the dataset
img_array = np.array(img)

# Plot it
plt.imshow(img_array, extent=[-125, -113, 32, 43], aspect='auto', alpha=0.6)
# Scatter plot
scatter = plt.scatter(
    housing['longitude'],
    housing['latitude'],
    c=housing['median_house_value'],
    cmap='viridis',
    s=housing['population']/100,
    alpha=0.5
)

# colorbar
cbar = plt.colorbar(scatter)
cbar.set_label('Median House Value')

# add title and labels
plt.title('Spatial Ditribution of Median House value',fontsize=16,fontweight='bold')
plt.xlabel('Longitude',fontsize=12)
plt.ylabel('Latitude',fontsize=12)
plt.tight_layout()
plt.show()








housing['bedrooms_per_rooms'] = housing['total_bedrooms'] / housing['total_rooms']


housing['rooms_per_household'] = housing['total_rooms'] / housing['households']


housing['population_per_household'] = housing['population'] / housing['households']


housing_num = housing.select_dtypes(include='number')
housing_num.corr()['median_house_value'].sort_values(ascending=False)





# Group by ocean_proximity
grouped = housing.groupby('ocean_proximity').agg({
    'population': 'sum',
    'median_house_value': 'mean'
}).sort_values(by='population', ascending=False)

# Compute percentage share
grouped['percentage'] = (grouped['population'] / grouped['population'].sum()) * 100

# Filter out categories with < 1% population share (e.g., ISLAND)
filtered = grouped[grouped['percentage'] >= 1]

# Data for pie chart
labels = filtered.index
sizes = filtered['population']
avg_house_values = grouped['median_house_value']

# Format labels to include average house value
formatted_labels = [
    f"{label}\nAvg $: ({int(avg_val):,})" 
    for label, avg_val in zip(labels, avg_house_values)
]

# Plot
plt.figure(figsize=(10, 7))
plt.pie(
    sizes,
    labels=formatted_labels,
    autopct='%1.1f%%',
    colors=sns.color_palette('Set2', len(labels)),
    explode=[0.05]*len(labels),
    startangle=140
)
plt.title("Population Share by Ocean Proximity\n(With Avg Median House Value)", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
plt.savefig('pie chart of Population Share by Ocean Proximity.png')














housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"]











from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from sklearn.compose import ColumnTransformer
from custom_transformers import LogTransformer, CombinedAttributesAdder

#  Detect numeric and categorical columns
num_features = housing.select_dtypes(include='number').columns.tolist()
cat_features = housing.select_dtypes(include='object').columns.tolist()
log_features = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']

# Build numeric pipeline
num_pipeline = Pipeline([
    ('log_transform', LogTransformer(log_features)),
    ('attribs_adder', CombinedAttributesAdder()),           # Add new domain-specific features
    ('imputer', SimpleImputer(strategy="median")),          # Fill missing values
    ('scaler', StandardScaler())                            # Scale the features
])

# 4. Full preprocessing pipeline
full_pipeline = ColumnTransformer([
    ('num', num_pipeline, num_features),
    ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)
])

# 5. Apply the pipeline to the data
housing_prepared = full_pipeline.fit_transform(housing) # return array

log_cols = [col + '_log' for col in log_features]  # 5 log-transformed columns
domain_cols = ['rooms_per_household', 'population_per_household', 'bedrooms_per_rooms']

# encoded column name
cat_encoder = full_pipeline.named_transformers_['cat']
cat_col_names = cat_encoder.get_feature_names_out(cat_features)

# Get the numeric columns that were not log-transformed
non_log_num_cols = [col for col in num_features if col not in log_features]

# Combine all numeric output columns
numeric_output_cols = log_cols + domain_cols + non_log_num_cols

all_feature_names = numeric_output_cols + list(cat_col_names) + log_features

# Debug check
print("Expected columns:", len(all_feature_names))  # Should be 21
print("Prepared data shape:", housing_prepared.shape)  # Should be (16512, 21)



housing_df = pd.DataFrame(housing_prepared, columns=all_feature_names)








from sklearn.linear_model import LinearRegression


# create the model
model = LinearRegression()
model.fit(housing_prepared,housing_labels)

# predict the model on some data
some_data = housing.iloc[:5]
some_labels = housing_labels[:5]

prepared_data = full_pipeline.transform(some_data)


print("Prediction",model.predict(prepared_data))
print("Actual Labels\n", some_labels)





from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Dictionary of models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "HistGradientBoosting": HistGradientBoostingRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, verbosity=0)
}

# Store model performance
model_performance = {}

# Loop through models
for name, model in models.items():
    model.fit(housing_prepared, housing_labels)
    predictions = model.predict(housing_prepared)

    mse = mean_squared_error(housing_labels, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(housing_labels, predictions)
    r2 = r2_score(housing_labels, predictions)

    # Store rounded results
    model_performance[name] = {
        "MSE": round(mse, 2),
        "RMSE": round(rmse, 2),
        "MAE": round(mae, 2),
        "R²": round(r2, 4)
    }

# Convert to DataFrame
performance_df = pd.DataFrame(model_performance).T.sort_values(by="RMSE")

# Display nicely
print("🔍 Model Performance Table:")
display(performance_df)





# 1. Get model names and scores
models = list(model_performance.keys())
rmse_score = [model_performance[m]['RMSE'] for m in models]
r2_scores = [model_performance[m]['R²'] for m in models]

# 2. Sort by RMSE
sorted_indices = np.argsort(rmse_score)
sorted_models = [models[i] for i in sorted_indices]
sorted_rmse = [rmse_score[i] for i in sorted_indices]
sorted_r2 = [r2_scores[i] for i in sorted_indices]

# 3. Create figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# --- Plot RMSE ---
sns.barplot(x=sorted_models, y=sorted_rmse, palette="Blues_d", ax=axes[0])
axes[0].set_title("RMSE Comparison of Models", fontsize=14, fontweight='bold')
axes[0].set_ylabel("RMSE")
axes[0].set_xlabel("Models")
axes[0].tick_params(axis='x', rotation=60)
axes[0].grid(True)

# Add value labels on top
for i, v in enumerate(sorted_rmse):
    axes[0].text(i, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=10)

# --- Plot R² ---
sns.barplot(x=sorted_models, y=sorted_r2, palette="Blues_d", ax=axes[1])
axes[1].set_title("R² Comparison of Models", fontsize=14, fontweight='bold')
axes[1].set_ylabel("R²")
axes[1].set_xlabel("Models")
axes[1].tick_params(axis='x', rotation=60)
axes[1].grid(True)

# Add value labels on top
for i, v in enumerate(sorted_r2):
    axes[1].text(i, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()
plt.savefig('Model comparison.png')








from sklearn.model_selection import RandomizedSearchCV


# define the parameter grid
param_dict = {
    'n_estimators' : [100, 200, 300, 500],
    'max_depth' : [None,10, 20, 30, 40, 50],
    'min_samples_split' : [2,5,10],
    'min_samples_leaf' : [1,2,4],
    'max_features' : ['auto','sqrt','log2']

}

# create the base model
rf = RandomForestRegressor(random_state=42)

# Randomized search setup
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dict,
    n_iter=20,   # try 20 random combination
    cv=5,        # 5 - fold cross validation
    verbose=2,
    random_state=42,
    n_jobs=-1,
    scoring='neg_root_mean_squared_error'  # You can also try 'neg_mean_squared_error'
)

# fit on training data
random_search.fit(housing_prepared,housing_labels)

# get the best model
best_model = random_search.best_estimator_

# prediction
final_predictons = best_model.predict(housing_prepared)

# Evaluation matrics
mse = mean_squared_error(housing_labels,final_predictons)
rmse = np.sqrt(mse)
mae = mean_absolute_error(housing_labels,final_predictons)
r2 = r2_score(housing_labels,final_predictons)

print("\n✅ Best Parameters Found:")
print(random_search.best_params_)

print("\n📊 Final Evaluation on Test Set:")
print(f"RMSE: {round(rmse, 2)}")
print(f"MAE : {round(mae, 2)}")
print(f"R²  : {round(r2, 4)}")





plt.figure(figsize=(16, 10))

# --- Plot 1: Actual vs Predicted ---
plt.subplot(1, 2, 1)
sns.scatterplot(x=housing_labels, y=final_predictons, color='royalblue', alpha=0.6)

# Perfect prediction line
plt.plot(
    [housing_labels.min(), housing_labels.max()],
    [housing_labels.min(), housing_labels.max()],
    color='red', linestyle='--', linewidth=2, label='Perfect Prediction'
)

plt.xlabel("Actual Median House Value")
plt.ylabel("Predicted Median House Value")
plt.title("Actual vs Predicted House Prices")
plt.legend()
plt.grid(True)

# --- Plot 2: Residuals ---
residuals = housing_labels - final_predictons

plt.subplot(1, 2, 2)
sns.scatterplot(x=final_predictons, y=residuals, color='orange', alpha=0.6)
plt.axhline(y=0, color='black', linestyle='--', linewidth=1)

plt.xlabel("Predicted Median House Value")
plt.ylabel("Residuals (Actual - Predicted)")
plt.title("Residual Plot")
plt.grid(True)

plt.tight_layout()
plt.show()
plt.savefig('Actual vs Predicted House prices and Residual plot')








# get featuer importance and column names
feature_importance = best_model.feature_importances_

# get feature names from final df `housing_df`
feature_names = housing_df.columns

# create the dataframe for visualizations
importance_df = pd.DataFrame({
    "Feature" : feature_names,
    "Importance":feature_importance
})

# sort values by Importance
importance_df = importance_df.sort_values(by='Importance',ascending=False)

# plot
plt.figure(figsize=(10, 8))
ax = sns.barplot(x="Importance", y="Feature", data=importance_df.head(20), palette="Set2")
plt.title("Top 20 Most Important Features (Random Forest)")
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.tight_layout()
plt.grid(True,axis='x', linestyle='--', alpha=0.5)

# Add value labels
for i, v in enumerate(importance_df.head(20)["Importance"]):
    ax.text(v + 0.002, i, f"{v:.4f}", color='black', va='center', fontsize=9)
    
plt.show()
plt.savefig('Top 20 Most Important Features.png')








# Drop target and make copy
housing_test = strat_test_set.drop("median_house_value", axis=1)
housing_test_labels = strat_test_set["median_house_value"].copy()

# Use full_pipeline used earlier
housing_test_prepared = full_pipeline.transform(housing_test)

# Predict with best model from RandomizedSearchCV
final_predictions = best_model.predict(housing_test_prepared)

# Evaluation metrics
mse = mean_squared_error(housing_test_labels, final_predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(housing_test_labels, final_predictions)
r2 = r2_score(housing_test_labels, final_predictions)

# Print results
print("✅ Final Evaluation on Test Set:")
print(f"RMSE: {round(rmse, 2)}")
print(f"MAE : {round(mae, 2)}")
print(f"R²  : {round(r2, 4)}")








def predict_house_price(model, pipeline, input_data):
    """
    Predicts median house value using a trained model and pipeline.

    Parameters:
    -----------
    model : trained estimator (e.g. best_rf)
        The trained regression model
    pipeline : fitted preprocessing pipeline
        The ColumnTransformer + custom steps
    input_data : dict
        Dictionary with feature names and values

    Returns:
    --------
    price_prediction : float
        Predicted median house value
    """
    # Step 1: Convert input dict to DataFrame
    input_df = pd.DataFrame([input_data])
    
    # Step 2: Apply preprocessing
    prepared_data = pipeline.transform(input_df)
    
    # Step 3: Predict using the model
    prediction = model.predict(prepared_data)
    
    # Step 4: Return prediction rounded to 2 decimal places
    return round(prediction[0], 2)

# Example input (replace with actual values)
new_house = {
    'longitude': -122.23,
    'latitude': 37.88,
    'housing_median_age': 41.0,
    'total_rooms': 880.0,
    'total_bedrooms': 129.0,
    'population': 322.0,
    'households': 126.0,
    'median_income': 8.3252,
    'ocean_proximity': 'NEAR BAY'
}

# Predict the price
predicted_price = predict_house_price(best_model, full_pipeline, new_house)

print(f"🏠 Predicted Median House Value: ${predicted_price}")





import joblib
from custom_transformers import LogTransformer, CombinedAttributesAdder

joblib.dump(best_model, "final_random_forest_model.pkl")
joblib.dump(full_pipeline, "full_pipeline.pkl")




